{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Lab\n",
    "\n",
    "You will find in this notebook some scrapy exercises to practise your scraping skills.\n",
    "\n",
    "**Tips:**\n",
    "\n",
    "- Check the response status code for each request to ensure you have obtained the intended content.\n",
    "- Print the response text in each request to understand the kind of info you are getting and its format.\n",
    "- Check for patterns in the response text to extract the data/info requested in each question.\n",
    "- Visit the urls below and take a look at their source code through Chrome DevTools. You'll need to identify the html tags, special class names, etc used in the html content you are expected to extract.\n",
    "\n",
    "**Resources**:\n",
    "- [Requests library](http://docs.python-requests.org/en/master/#the-user-guide)\n",
    "- [Beautiful Soup Doc](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
    "- [Urllib](https://docs.python.org/3/library/urllib.html#module-urllib)\n",
    "- [re lib](https://docs.python.org/3/library/re.html)\n",
    "- [lxml lib](https://lxml.de/)\n",
    "- [Scrapy](https://scrapy.org/)\n",
    "- [List of HTTP status codes](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes)\n",
    "- [HTML basics](http://www.simplehtmlguide.com/cheatsheet.php)\n",
    "- [CSS basics](https://www.cssbasics.com/#page_start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below are the libraries and modules you may need. `requests`,  `BeautifulSoup` and `pandas` are already imported for you. If you prefer to use additional libraries feel free to do it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Download, parse (using BeautifulSoup), and print the content from the Trending Developers page from GitHub:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/developers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = response.content\n",
    "soup = BeautifulSoup(html)\n",
    "# soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the names of the trending developers retrieved in the previous step.\n",
    "\n",
    "Your output should be a Python list of developer names. Each name should not contain any html tag.\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Find out the html tag and class names used for the developer names. You can achieve this using Chrome DevTools.\n",
    "\n",
    "1. Use BeautifulSoup to extract all the html elements that contain the developer names.\n",
    "\n",
    "1. Use string manipulation techniques to replace whitespaces and linebreaks (i.e. `\\n`) in the *text* of each html element. Use a list to store the clean names.\n",
    "\n",
    "1. Print the list of names.\n",
    "\n",
    "Your output should look like below:\n",
    "\n",
    "```\n",
    "['trimstray (@trimstray)',\n",
    " 'joewalnes (JoeWalnes)',\n",
    " 'charlax (Charles-AxelDein)',\n",
    " 'ForrestKnight (ForrestKnight)',\n",
    " 'revery-ui (revery-ui)',\n",
    " 'alibaba (Alibaba)',\n",
    " 'Microsoft (Microsoft)',\n",
    " 'github (GitHub)',\n",
    " 'facebook (Facebook)',\n",
    " 'boazsegev (Bo)',\n",
    " 'google (Google)',\n",
    " 'cloudfetch',\n",
    " 'sindresorhus (SindreSorhus)',\n",
    " 'tensorflow',\n",
    " 'apache (TheApacheSoftwareFoundation)',\n",
    " 'DevonCrawford (DevonCrawford)',\n",
    " 'ARMmbed (ArmMbed)',\n",
    " 'vuejs (vuejs)',\n",
    " 'fastai (fast.ai)',\n",
    " 'QiShaoXuan (Qi)',\n",
    " 'joelparkerhenderson (JoelParkerHenderson)',\n",
    " 'torvalds (LinusTorvalds)',\n",
    " 'CyC2018',\n",
    " 'komeiji-satori (神楽坂覚々)',\n",
    " 'script-8']\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = soup.find_all('h1', attrs={'class' : 'h3 lh-condensed'})\n",
    "names = [item.text.strip() for item in names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "usernames = soup.find_all('p', attrs={'class' : 'f4 text-normal mb-1'})\n",
    "usernames = [item.text.strip() for item in usernames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Ariel Mashraki', 'a8m'),\n",
       " ('Anitaa Murthy', 'anitaa1990'),\n",
       " ('Vadim Dalecky', 'streamich'),\n",
       " ('Hamel Husain', 'hamelsmu'),\n",
       " ('Emmanuelle Gouillart', 'emmanuelle'),\n",
       " ('Artur Bosch', 'arturbosch'),\n",
       " ('Jonah Williams', 'jonahwilliams'),\n",
       " ('Jyoti Puri', 'jpuri'),\n",
       " ('shimat', 'shimat'),\n",
       " ('Jack Zampolin', 'jackzampolin'),\n",
       " ('Alon Zakai', 'kripken'),\n",
       " ('Jake Wharton', 'JakeWharton'),\n",
       " ('Carl Lerche', 'carllerche'),\n",
       " ('isaacs', 'isaacs'),\n",
       " ('Orne Brocaar', 'brocaar'),\n",
       " ('Ulises Gascón', 'UlisesGascon'),\n",
       " ('Diego Muracciole', 'diegomura'),\n",
       " ('Stéphane Lepin', 'Palakis'),\n",
       " ('Matthieu Napoli', 'mnapoli'),\n",
       " ('Chet Corcos', 'ccorcos'),\n",
       " ('Remi Rousselet', 'rrousselGit'),\n",
       " ('Martin Monperrus', 'monperrus'),\n",
       " ('Florian Rival', '4ian'),\n",
       " ('Nico Schlömer', 'nschloe'),\n",
       " ('Satyajit Sahoo', 'satya164')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(zip(names,usernames))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the trending Python repositories in GitHub.\n",
    "\n",
    "The steps to solve this problem is similar to the previous one except that you need to find out the repository names instead of developer names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://github.com/trending/python?since=daily'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pennsignals / chime',\n",
       " 'matrix-org / synapse',\n",
       " 'huawei-noah / AdderNet',\n",
       " 'lazyprogrammer / machine_learning_examples',\n",
       " 'VertaAI / modeldb',\n",
       " 'stanfordnlp / stanza',\n",
       " 'geerlingguy / ansible-for-devops',\n",
       " 'dmlc / gluon-cv',\n",
       " 'ansible / ansible',\n",
       " 'ExpDev07 / coronavirus-tracker-api',\n",
       " 'gto76 / python-cheatsheet',\n",
       " 'ycm-core / YouCompleteMe',\n",
       " 'localstack / localstack',\n",
       " 'vishnubob / wait-for-it',\n",
       " 'OpenMined / PySyft',\n",
       " 'lark-parser / lark',\n",
       " 'vinta / awesome-python',\n",
       " 'huggingface / transformers',\n",
       " 'python-telegram-bot / python-telegram-bot',\n",
       " 'tensorflow / tfx',\n",
       " 'bitcoin / bips',\n",
       " 'fishtown-analytics / dbt',\n",
       " 'django / django',\n",
       " 'python / mypy',\n",
       " 'pypa / pip']"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repos = soup.find_all('h1', attrs={'class':'h3 lh-condensed'})\n",
    "repos = [item.text.strip().replace('/\\n\\n\\n\\n     ', '/') for item in repos]\n",
    "repos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display all the image links from Walt Disney wikipedia page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/Walt_Disney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Cscr-featured.svg/20px-Cscr-featured.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/1/1b/Semi-protection-shackle.svg/20px-Semi-protection-shackle.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/df/Walt_Disney_1946.JPG/220px-Walt_Disney_1946.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/87/Walt_Disney_1942_signature.svg/150px-Walt_Disney_1942_signature.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/c4/Walt_Disney_envelope_ca._1921.jpg/220px-Walt_Disney_envelope_ca._1921.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Newman_Laugh-O-Gram_%281921%29.webm/220px-seek%3D2-Newman_Laugh-O-Gram_%281921%29.webm.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/0/0d/Trolley_Troubles_poster.jpg/170px-Trolley_Troubles_poster.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/7/71/Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg/170px-Walt_Disney_and_his_cartoon_creation_%22Mickey_Mouse%22_-_National_Board_of_Review_Magazine.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4e/Steamboat-willie.jpg/170px-Steamboat-willie.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/5/57/Walt_Disney_1935.jpg/170px-Walt_Disney_1935.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg/220px-Walt_Disney_Snow_white_1937_trailer_screenshot_%2813%29.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/15/Disney_drawing_goofy.jpg/170px-Disney_drawing_goofy.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/13/DisneySchiphol1951.jpg/220px-DisneySchiphol1951.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/WaltDisneyplansDisneylandDec1954.jpg/220px-WaltDisneyplansDisneylandDec1954.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Walt_disney_portrait_right.jpg/170px-Walt_disney_portrait_right.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Walt_Disney_Grave.JPG/170px-Walt_Disney_Grave.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Roy_O._Disney_with_Company_at_Press_Conference.jpg/170px-Roy_O._Disney_with_Company_at_Press_Conference.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Disney_Display_Case.JPG/170px-Disney_Display_Case.JPG',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/6/6c/Disney1968.jpg/170px-Disney1968.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/4/44/The_Walt_Disney_Company_Logo.svg/120px-The_Walt_Disney_Company_Logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/d/da/Animation_disc.svg/30px-Animation_disc.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/6/69/P_vip.svg/29px-P_vip.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/1/1a/Magic_Kingdom_castle.jpg/24px-Magic_Kingdom_castle.jpg',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/e/e7/Video-x-generic.svg/30px-Video-x-generic.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/a/a3/Flag_of_Los_Angeles_County%2C_California.svg/30px-Flag_of_Los_Angeles_County%2C_California.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Blank_television_set.svg/30px-Blank_television_set.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/a/a4/Flag_of_the_United_States.svg/30px-Flag_of_the_United_States.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/4/4a/Commons-logo.svg/22px-Commons-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Wikiquote-logo.svg/25px-Wikiquote-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Wikidata-logo.svg/30px-Wikidata-logo.svg.png',\n",
       " '//upload.wikimedia.org/wikipedia/en/thumb/8/8a/OOjs_UI_icon_edit-ltr-progressive.svg/10px-OOjs_UI_icon_edit-ltr-progressive.svg.png',\n",
       " '//en.wikipedia.org/wiki/Special:CentralAutoLogin/start?type=1x1',\n",
       " '/static/images/wikimedia-button.png',\n",
       " '/static/images/poweredby_mediawiki_88x31.png']"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = soup.find_all('img')\n",
    "[item['src'] for item in imgs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retrieve an arbitary Wikipedia page of \"Python\" and create a list of links on that page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url ='https://en.wikipedia.org/wiki/Python' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)\n",
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-f7e09c89c9e6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlinks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msoup\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'href'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"^http\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "links = [item['href'] for item in soup.find_all('a', attrs={'href': re.compile(\"^http\")})]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the number of titles that have changed in the United States Code since its last release point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'http://uscode.house.gov/download/download.shtml'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tit = soup.find_all('div', attrs={'class':'usctitlechanged'})\n",
    "len([item.text.strip() for item in tit])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find a Python list with the top ten FBI's Most Wanted names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.fbi.gov/wanted/topten'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['RAFAEL CARO-QUINTERO',\n",
       " 'ROBERT WILLIAM FISHER',\n",
       " 'BHADRESHKUMAR CHETANBHAI PATEL',\n",
       " 'ALEJANDRO ROSALES CASTILLO',\n",
       " 'ARNOLDO JIMENEZ',\n",
       " 'YASER ABDEL SAID',\n",
       " 'JASON DEREK BROWN',\n",
       " 'ALEXIS FLORES',\n",
       " 'EUGENE PALMER',\n",
       " 'SANTIAGO VILLALBA MEDEROS']"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names = soup.find_all('h3', attrs={'class':'title'})\n",
    "[item.text.strip() for item in names]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Display the 20 latest earthquakes info (date, time, latitude, longitude and region name) by the EMSC as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.emsc-csem.org/Earthquake/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "etq = pd.read_html(requests.get(url).content)[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "etq = etq[['Date & Time UTC', 'Latitude degrees', 'Longitude degrees', 'Last update [-]']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "etq.rename(columns={'Latitude degrees': 'Latitude', 'Longitude degrees':'Longitude', 'Last update [-]':'Region Name'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "etq.columns = etq.columns.droplevel(level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date &amp; Time UTC</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Region Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2020-03-19 00:36:05.135min ago</td>\n",
       "      <td>40.33</td>\n",
       "      <td>N</td>\n",
       "      <td>124.39</td>\n",
       "      <td>W</td>\n",
       "      <td>OFFSHORE NORTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2020-03-19 00:16:32.855min ago</td>\n",
       "      <td>38.51</td>\n",
       "      <td>N</td>\n",
       "      <td>15.20</td>\n",
       "      <td>E</td>\n",
       "      <td>SICILY, ITALY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2020-03-18 23:54:26.01hr 17min ago</td>\n",
       "      <td>7.07</td>\n",
       "      <td>S</td>\n",
       "      <td>108.70</td>\n",
       "      <td>E</td>\n",
       "      <td>JAVA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2020-03-18 23:50:06.01hr 21min ago</td>\n",
       "      <td>0.17</td>\n",
       "      <td>S</td>\n",
       "      <td>132.40</td>\n",
       "      <td>E</td>\n",
       "      <td>NEAR N COAST OF PAPUA, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2020-03-18 23:34:48.71hr 36min ago</td>\n",
       "      <td>41.79</td>\n",
       "      <td>N</td>\n",
       "      <td>19.92</td>\n",
       "      <td>E</td>\n",
       "      <td>ALBANIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2020-03-18 23:33:05.01hr 38min ago</td>\n",
       "      <td>34.73</td>\n",
       "      <td>S</td>\n",
       "      <td>71.75</td>\n",
       "      <td>W</td>\n",
       "      <td>LIBERTADOR O'HIGGINS, CHILE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2020-03-18 23:17:47.01hr 53min ago</td>\n",
       "      <td>40.39</td>\n",
       "      <td>N</td>\n",
       "      <td>124.56</td>\n",
       "      <td>W</td>\n",
       "      <td>OFFSHORE NORTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2020-03-18 23:15:13.11hr 56min ago</td>\n",
       "      <td>40.77</td>\n",
       "      <td>N</td>\n",
       "      <td>112.09</td>\n",
       "      <td>W</td>\n",
       "      <td>WASATCH FRONT URBAN AREA, UTAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2020-03-18 23:14:41.91hr 56min ago</td>\n",
       "      <td>17.87</td>\n",
       "      <td>N</td>\n",
       "      <td>66.98</td>\n",
       "      <td>W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2020-03-18 23:07:21.42hr 04min ago</td>\n",
       "      <td>38.51</td>\n",
       "      <td>N</td>\n",
       "      <td>44.51</td>\n",
       "      <td>E</td>\n",
       "      <td>TURKEY-IRAN BORDER REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2020-03-18 23:04:27.42hr 07min ago</td>\n",
       "      <td>17.92</td>\n",
       "      <td>N</td>\n",
       "      <td>66.88</td>\n",
       "      <td>W</td>\n",
       "      <td>PUERTO RICO REGION</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2020-03-18 22:51:50.02hr 19min ago</td>\n",
       "      <td>10.49</td>\n",
       "      <td>S</td>\n",
       "      <td>115.23</td>\n",
       "      <td>E</td>\n",
       "      <td>SOUTH OF BALI, INDONESIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2020-03-18 22:46:29.02hr 25min ago</td>\n",
       "      <td>40.75</td>\n",
       "      <td>N</td>\n",
       "      <td>112.08</td>\n",
       "      <td>W</td>\n",
       "      <td>WASATCH FRONT URBAN AREA, UTAH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2020-03-18 22:37:48.82hr 33min ago</td>\n",
       "      <td>36.55</td>\n",
       "      <td>N</td>\n",
       "      <td>121.11</td>\n",
       "      <td>W</td>\n",
       "      <td>CENTRAL CALIFORNIA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2020-03-18 22:12:47.62hr 58min ago</td>\n",
       "      <td>36.46</td>\n",
       "      <td>N</td>\n",
       "      <td>98.77</td>\n",
       "      <td>W</td>\n",
       "      <td>OKLAHOMA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2020-03-18 22:10:47.53hr 00min ago</td>\n",
       "      <td>46.50</td>\n",
       "      <td>N</td>\n",
       "      <td>9.80</td>\n",
       "      <td>E</td>\n",
       "      <td>SWITZERLAND</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2020-03-18 22:08:20.33hr 03min ago</td>\n",
       "      <td>40.31</td>\n",
       "      <td>N</td>\n",
       "      <td>124.40</td>\n",
       "      <td>W</td>\n",
       "      <td>OFFSHORE NORTHERN CALIFORNIA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       Date & Time UTC Latitude Latitude Longitude Longitude  \\\n",
       "0       2020-03-19 00:36:05.135min ago    40.33        N    124.39         W   \n",
       "1       2020-03-19 00:16:32.855min ago    38.51        N     15.20         E   \n",
       "2                                  NaN      NaN      NaN       NaN       NaN   \n",
       "3                                  NaN      NaN      NaN       NaN       NaN   \n",
       "4                                  NaN      NaN      NaN       NaN       NaN   \n",
       "5   2020-03-18 23:54:26.01hr 17min ago     7.07        S    108.70         E   \n",
       "6   2020-03-18 23:50:06.01hr 21min ago     0.17        S    132.40         E   \n",
       "7   2020-03-18 23:34:48.71hr 36min ago    41.79        N     19.92         E   \n",
       "8   2020-03-18 23:33:05.01hr 38min ago    34.73        S     71.75         W   \n",
       "9   2020-03-18 23:17:47.01hr 53min ago    40.39        N    124.56         W   \n",
       "10  2020-03-18 23:15:13.11hr 56min ago    40.77        N    112.09         W   \n",
       "11  2020-03-18 23:14:41.91hr 56min ago    17.87        N     66.98         W   \n",
       "12  2020-03-18 23:07:21.42hr 04min ago    38.51        N     44.51         E   \n",
       "13  2020-03-18 23:04:27.42hr 07min ago    17.92        N     66.88         W   \n",
       "14  2020-03-18 22:51:50.02hr 19min ago    10.49        S    115.23         E   \n",
       "15  2020-03-18 22:46:29.02hr 25min ago    40.75        N    112.08         W   \n",
       "16  2020-03-18 22:37:48.82hr 33min ago    36.55        N    121.11         W   \n",
       "17  2020-03-18 22:12:47.62hr 58min ago    36.46        N     98.77         W   \n",
       "18  2020-03-18 22:10:47.53hr 00min ago    46.50        N      9.80         E   \n",
       "19  2020-03-18 22:08:20.33hr 03min ago    40.31        N    124.40         W   \n",
       "\n",
       "                         Region Name  \n",
       "0       OFFSHORE NORTHERN CALIFORNIA  \n",
       "1                      SICILY, ITALY  \n",
       "2                                NaN  \n",
       "3                                NaN  \n",
       "4                                NaN  \n",
       "5                    JAVA, INDONESIA  \n",
       "6   NEAR N COAST OF PAPUA, INDONESIA  \n",
       "7                            ALBANIA  \n",
       "8        LIBERTADOR O'HIGGINS, CHILE  \n",
       "9       OFFSHORE NORTHERN CALIFORNIA  \n",
       "10    WASATCH FRONT URBAN AREA, UTAH  \n",
       "11                PUERTO RICO REGION  \n",
       "12         TURKEY-IRAN BORDER REGION  \n",
       "13                PUERTO RICO REGION  \n",
       "14          SOUTH OF BALI, INDONESIA  \n",
       "15    WASATCH FRONT URBAN AREA, UTAH  \n",
       "16                CENTRAL CALIFORNIA  \n",
       "17                          OKLAHOMA  \n",
       "18                       SWITZERLAND  \n",
       "19      OFFSHORE NORTHERN CALIFORNIA  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etq.loc[:19,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the number of tweets by a given Twitter account.\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the number of tweets for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_tweets():\n",
    "    url = 'https://twitter.com/'\n",
    "    user = str(input('Insert a twitter username: '))\n",
    "    url = url + user\n",
    "    response = requests.get(url)\n",
    "    try:\n",
    "        response.status_code == 200\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html)\n",
    "        tweets_nav = soup.find('li', {'class':'ProfileNav-item ProfileNav-item--tweets is-active'})\n",
    "        tweets = int(tweets_nav.find('a').find('span', {'class':'ProfileNav-value'})['data-count'])\n",
    "        return tweets\n",
    "    except:\n",
    "        print('Username not found!')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert a twitter username: ndiuqbdqosidxhqieubc\n",
      "Username not found!\n",
      "Insert a twitter username: g1\n",
      "674597 tweets\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    tweets = count_tweets()\n",
    "    if tweets != False:\n",
    "        print(f'{tweets} tweets')\n",
    "        break        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of followers of a given twitter account\n",
    "Ask the user for the handle (@handle) of a twitter account. You will need to include a ***try/except block*** for account names not found. \n",
    "<br>***Hint:*** the program should count the followers for any provided account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_followers():\n",
    "    url = 'https://twitter.com/'\n",
    "    user = str(input('Insert a twitter username: '))\n",
    "    url = url + user\n",
    "    response = requests.get(url)\n",
    "    try:\n",
    "        response.status_code == 200\n",
    "        html = response.content\n",
    "        soup = BeautifulSoup(html)\n",
    "        tweets_nav = soup.find('li', {'class':'ProfileNav-item ProfileNav-item--followers'})\n",
    "        followers = int(tweets_nav.find('a').find('span', {'class':'ProfileNav-value'})['data-count'])\n",
    "        return followers\n",
    "    except:\n",
    "        print('Username not found!')\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insert a twitter username: nidweuf\n",
      "Username not found!\n",
      "Insert a twitter username: g1\n",
      "10853555 followers\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    followers = count_followers()\n",
    "    if followers != False:\n",
    "        print(f'{followers} followers')\n",
    "        break "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List all language names and number of related articles in the order they appear in wikipedia.org."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://www.wikipedia.org/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['English 6029000+ articles',\n",
       " 'Español 1581000+ artículos',\n",
       " '日本語 1193000+ 記事',\n",
       " 'Deutsch 2406000+ Artikel',\n",
       " 'Русский 1602000+ статей',\n",
       " 'Français 2187000+ articles',\n",
       " 'Italiano 1588000+ voci',\n",
       " '中文 1101000+ 條目',\n",
       " 'Português 1023000+ artigos',\n",
       " 'Polski 1388000+ haseł']"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lang = soup.find_all('div', attrs={'class':'central-featured-lang'})\n",
    "[item.text.strip().replace('\\n', ' ').replace('\\xa0', '') for item in lang]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A list with the different kind of datasets available in data.gov.uk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://data.gov.uk/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Business and economy',\n",
       " 'Crime and justice',\n",
       " 'Defence',\n",
       " 'Education',\n",
       " 'Environment',\n",
       " 'Government',\n",
       " 'Government spending',\n",
       " 'Health',\n",
       " 'Mapping',\n",
       " 'Society',\n",
       " 'Towns and cities',\n",
       " 'Transport']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds = soup.find_all('h2')\n",
    "[item.text for item in ds]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the top 10 languages by number of native speakers stored in a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise\n",
    "url = 'https://en.wikipedia.org/wiki/List_of_languages_by_number_of_native_speakers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Language</th>\n",
       "      <th>Speakers(millions)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mandarin Chinese</td>\n",
       "      <td>918.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Spanish</td>\n",
       "      <td>480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>English</td>\n",
       "      <td>379.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hindi (Sanskritised Hindustani)[9]</td>\n",
       "      <td>341.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Bengali</td>\n",
       "      <td>228.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Portuguese</td>\n",
       "      <td>221.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Russian</td>\n",
       "      <td>154.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Japanese</td>\n",
       "      <td>128.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Western Punjabi[10]</td>\n",
       "      <td>92.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Marathi</td>\n",
       "      <td>83.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Language  Speakers(millions)\n",
       "0                    Mandarin Chinese               918.0\n",
       "1                             Spanish               480.0\n",
       "2                             English               379.0\n",
       "3  Hindi (Sanskritised Hindustani)[9]               341.0\n",
       "4                             Bengali               228.0\n",
       "5                          Portuguese               221.0\n",
       "6                             Russian               154.0\n",
       "7                            Japanese               128.0\n",
       "8                 Western Punjabi[10]                92.7\n",
       "9                             Marathi                83.1"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_lang = pd.read_html(url)[0]\n",
    "top_lang = top_lang.sort_values(by='Speakers(millions)', ascending=False).head(10)\n",
    "top_lang[['Language', 'Speakers(millions)']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "#### Scrape a certain number of tweets of a given Twitter account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "# You will need to add the account credentials to this url\n",
    "url = 'https://twitter.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Twitter username: g1\n",
      "How many tweets do you want to see? (Max: 20) 20\n",
      "\n",
      "\n",
      "#Coronavírus: 662 brasileiros retidos no Peru retornam ao país nesta sexta, informa Ministério do Turismo https://glo.bo/2Ws3yQApic.twitter.com/akhALZH8zM\n",
      "\n",
      "\n",
      "Coronavírus pode infectar a mesma pessoa duas vezes? A pergunta que intriga cientistas https://glo.bo/2wl5OhN #G1pic.twitter.com/9phVx1Yxzc\n",
      "\n",
      "\n",
      "Farmácia, mercado e banco: vizinhos oferecem ajuda a moradores isolados pelo #coronavírus https://glo.bo/2UnEkzV #G1pic.twitter.com/j6UAjBGdA5\n",
      "\n",
      "\n",
      "#Coronavírus: o que diz o modelo matemático que levou o Reino Unido a mudar radicalmente combate à Covid-19 https://glo.bo/2Qu7T1O #G1pic.twitter.com/XU7r2EfSci\n",
      "\n",
      "\n",
      "Festivais sem sair de casa: lives são opção para quem quer ver shows durante quarentena https://glo.bo/399iohH #G1pic.twitter.com/fUmJ2FgwgF\n",
      "\n",
      "\n",
      "\"Alguns consideram que são pequenos heróis quando violam as regras. Mas não, são imbecis e sobretudo uma ameaça para si mesmos\", declarou nesta quinta-feira o ministro do Interior da França, Christophe Castaner https://glo.bo/2x4eitH #G1 #Coronavírus\n",
      "\n",
      "\n",
      "PRF prorroga prazo para recorrer de multas aplicadas pela instituição https://glo.bo/3b9dSkp #G1pic.twitter.com/FdPkA7b2qK\n",
      "\n",
      "\n",
      "Na Europa, o confinamento priva as famílias dos velórios de parentes https://glo.bo/390Gcnx #G1 #Coronavíruspic.twitter.com/9PDoe690rY\n",
      "\n",
      "\n",
      "Portugal declara estado de emergência por causa do coronavírus https://glo.bo/2w9xgiB #G1pic.twitter.com/qQCAe5IxXu\n",
      "\n",
      "\n",
      "Brasil fecha fronteiras terrestres, mas libera cargas e ações humanitárias https://glo.bo/2U0KneV #G1pic.twitter.com/SZpkrUDZrF\n",
      "\n",
      "\n",
      "Missa do padroeiro do Ceará é transmitida on-line e sem fiéis na igreja https://glo.bo/2Un84wP #G1 #Coronavíruspic.twitter.com/Wnk3SI98mn\n",
      "\n",
      "\n",
      "#Coronavírus: Prefeitura do Rio determina o fechamento de praças gradeadas para evitar aglomerações https://glo.bo/3d5IBka #G1 #CoronavírusnoBrasilpic.twitter.com/6SFKUSlWBr\n",
      "\n",
      "\n",
      "Governo do RJ confirma a segunda morte por coronavírus; vítima é homem de Niterói https://glo.bo/3dcoTU3 #G1pic.twitter.com/U932BKVDFl\n",
      "\n",
      "\n",
      "#Coronavírus: Brasil tem 5 mortes e mais de 530 infectados: veja números por estado https://glo.bo/3baWa01 #G1pic.twitter.com/pS3amQnLlW\n",
      "\n",
      "\n",
      "Para alertar sobre combate ao coronavírus, Turma da Mônica coloca Cascão para lavar as mãos https://glo.bo/2IYwIyD #G1pic.twitter.com/ltuXyeKQUk\n",
      "\n",
      "\n",
      "Príncipe de Mônaco tem resultado positivo para coronavírus https://glo.bo/3b819i0 #G1pic.twitter.com/7VjUC17yBV\n",
      "\n",
      "\n",
      "#Coronavírus: arte reflete impacto mundial da doença; veja outras fotos https://glo.bo/3b6mUPa #G1pic.twitter.com/QmHk0jxap0\n",
      "\n",
      "\n",
      "Exército da Itália retira corpos de cidade sobrecarregada pelo coronavírus https://glo.bo/33wOfYi #G1pic.twitter.com/40gwZ2J3ea\n",
      "\n",
      "\n",
      "China não tem novos casos de transmissão local de Covid-19 pela primeira vez https://glo.bo/393V4ll #G1pic.twitter.com/XlXYzD8EPa\n",
      "\n",
      "\n",
      "Quanto tempo o coronavírus sobrevive nas superfícies? Estudo aponta que plástico e aço ampliam a sobrevida https://glo.bo/3dgjQ58 #G1 #Coronavírus #CoronavírusnoBrasilpic.twitter.com/KO5qDDv2AA\n"
     ]
    }
   ],
   "source": [
    "user = str(input('Twitter username: '))\n",
    "qty_tweets = int(input('How many tweets do you want to see? (Max: 20) '))\n",
    "response = requests.get(url + user)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)\n",
    "tweets = soup.find_all('p', attrs={'class':'TweetTextSize'})\n",
    "for i in range(qty_tweets):\n",
    "    print('\\n')\n",
    "    tweet = tweets[i+1].text.replace('\\xa0', '')\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display IMDB's top 250 data (movie name, initial release, director name and stars) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise \n",
    "url = 'https://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank &amp; Title</th>\n",
       "      <th>IMDb Rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.  Um Sonho de Liberdade  (1994)</td>\n",
       "      <td>9.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.  O Poderoso Chefão  (1972)</td>\n",
       "      <td>9.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.  O Poderoso Chefão II  (1974)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.  Batman: O Cavaleiro das Trevas  (2008)</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.  12 Homens e uma Sentença  (1957)</td>\n",
       "      <td>8.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>246.  Um Contratempo  (2016)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>247.  Guardiões da Galáxia  (2014)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>248.  A Chantagem  (2015)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>249.  Feitiço do Tempo  (1993)</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>250.  Neon Genesis Evangelion: O Fim do Evange...</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          Rank & Title  IMDb Rating\n",
       "0                    1.  Um Sonho de Liberdade  (1994)          9.2\n",
       "1                        2.  O Poderoso Chefão  (1972)          9.1\n",
       "2                     3.  O Poderoso Chefão II  (1974)          9.0\n",
       "3           4.  Batman: O Cavaleiro das Trevas  (2008)          9.0\n",
       "4                 5.  12 Homens e uma Sentença  (1957)          8.9\n",
       "..                                                 ...          ...\n",
       "245                       246.  Um Contratempo  (2016)          8.0\n",
       "246                 247.  Guardiões da Galáxia  (2014)          8.0\n",
       "247                          248.  A Chantagem  (2015)          8.0\n",
       "248                     249.  Feitiço do Tempo  (1993)          8.0\n",
       "249  250.  Neon Genesis Evangelion: O Fim do Evange...          8.0\n",
       "\n",
       "[250 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = pd.read_html(url)[0]\n",
    "movies[['Rank & Title', 'IMDb Rating']].head(250)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display the movie name, year and a brief summary of the top 10 random movies (IMDB) as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the url you will scrape in this exercise\n",
    "url = 'http://www.imdb.com/chart/top'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_html(url)[0]\n",
    "top_10_movies = movies[['Rank & Title']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(url)\n",
    "html = response.content\n",
    "soup = BeautifulSoup(html)\n",
    "links = soup.find_all('a', attrs={'href':re.compile(\"^/title\"), 'title':''})[0:10]\n",
    "links = ['https://www.imdb.com' + link['href'] for link in links]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = [requests.get(link) for link in links]\n",
    "html = [response.content for response in r]\n",
    "soup = [BeautifulSoup(ht) for ht in html]\n",
    "summary = [item.find_all('div', attrs={'class':'summary_text'})[0].text.strip() for item in soup]\n",
    "top_10_movies['Summary'] = pd.Series(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rank &amp; Title</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.  Um Sonho de Liberdade  (1994)</td>\n",
       "      <td>Two imprisoned men bond over a number of years...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.  O Poderoso Chefão  (1972)</td>\n",
       "      <td>The aging patriarch of an organized crime dyna...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.  O Poderoso Chefão II  (1974)</td>\n",
       "      <td>The early life and career of Vito Corleone in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.  Batman: O Cavaleiro das Trevas  (2008)</td>\n",
       "      <td>When the menace known as the Joker wreaks havo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.  12 Homens e uma Sentença  (1957)</td>\n",
       "      <td>A jury holdout attempts to prevent a miscarria...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6.  A Lista de Schindler  (1993)</td>\n",
       "      <td>In German-occupied Poland during World War II,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.  O Senhor dos Anéis: O Retorno do Rei  (2003)</td>\n",
       "      <td>Gandalf and Aragorn lead the World of Men agai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.  Pulp Fiction: Tempo de Violência  (1994)</td>\n",
       "      <td>The lives of two mob hitmen, a boxer, a gangst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9.  Três Homens em Conflito  (1966)</td>\n",
       "      <td>A bounty hunting scam joins two men in an unea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10.  O Senhor dos Anéis: A Sociedade do Anel  ...</td>\n",
       "      <td>A meek Hobbit from the Shire and eight compani...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        Rank & Title  \\\n",
       "0                  1.  Um Sonho de Liberdade  (1994)   \n",
       "1                      2.  O Poderoso Chefão  (1972)   \n",
       "2                   3.  O Poderoso Chefão II  (1974)   \n",
       "3         4.  Batman: O Cavaleiro das Trevas  (2008)   \n",
       "4               5.  12 Homens e uma Sentença  (1957)   \n",
       "5                   6.  A Lista de Schindler  (1993)   \n",
       "6   7.  O Senhor dos Anéis: O Retorno do Rei  (2003)   \n",
       "7       8.  Pulp Fiction: Tempo de Violência  (1994)   \n",
       "8                9.  Três Homens em Conflito  (1966)   \n",
       "9  10.  O Senhor dos Anéis: A Sociedade do Anel  ...   \n",
       "\n",
       "                                             Summary  \n",
       "0  Two imprisoned men bond over a number of years...  \n",
       "1  The aging patriarch of an organized crime dyna...  \n",
       "2  The early life and career of Vito Corleone in ...  \n",
       "3  When the menace known as the Joker wreaks havo...  \n",
       "4  A jury holdout attempts to prevent a miscarria...  \n",
       "5  In German-occupied Poland during World War II,...  \n",
       "6  Gandalf and Aragorn lead the World of Men agai...  \n",
       "7  The lives of two mob hitmen, a boxer, a gangst...  \n",
       "8  A bounty hunting scam joins two men in an unea...  \n",
       "9  A meek Hobbit from the Shire and eight compani...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_10_movies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the live weather report (temperature, wind speed, description and weather) of a given city."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter the city: sao paulo\n"
     ]
    }
   ],
   "source": [
    "#https://openweathermap.org/current\n",
    "city = input('Enter the city: ')\n",
    "url = 'http://api.openweathermap.org/data/2.5/weather?'+'q='+city+'&APPID=b35975e18dc93725acb092f7272cc6b8&units=metric'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "City: sao paulo\n",
      "Temperature: 28.59\n",
      "Wind Spped: 3.1\n",
      "Description: broken clouds\n",
      "Weather: Clouds\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "info = response.json()\n",
    "\n",
    "temp = info['main']['temp']\n",
    "wind_speed = info['wind']['speed']\n",
    "desc = info['weather'][0]['description']\n",
    "weather = info['weather'][0]['main']\n",
    "\n",
    "print(f'City: {city}\\nTemperature: {temp}\\nWind Spped: {wind_speed}\\nDescription: {desc}\\nWeather: {weather}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find the book name, price and stock availability as a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the url you will scrape in this exercise. \n",
    "# It is a fictional bookstore created to be scraped. \n",
    "url = 'http://books.toscrape.com/catalogue/'\n",
    "\n",
    "i = 0\n",
    "status_code = 200\n",
    "while status_code == 200:\n",
    "    i += 1\n",
    "    url = 'http://books.toscrape.com/catalogue/page-'+str(i)+'.html'\n",
    "    response = requests.get(url)\n",
    "    status_code = response.status_code\n",
    "    html = response.content\n",
    "    soup = BeautifulSoup(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [item['title'] for item in soup.find_all('a', attrs={'title':re.compile(\"^\")})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prices = [item.text for item in soup.find_all('p', attrs={'class':'price_color'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock = [item.text.strip() for item in soup.find_all('p', attrs={'class':'instock availability'})]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Movie</th>\n",
       "      <th>Price</th>\n",
       "      <th>Stock Availability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Movie, Price, Stock Availability]\n",
       "Index: []"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame([names, prices, stock]).T\n",
    "df = df.rename(columns={0:'Movie', 1:'Price', 2:'Stock Availability'})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
